{"ast":null,"code":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.AbstractTokenizer = void 0;\nconst peek_readable_1 = require(\"peek-readable\");\n/**\r\n * Core tokenizer\r\n */\nclass AbstractTokenizer {\n  constructor(fileInfo) {\n    /**\r\n     * Tokenizer-stream position\r\n     */\n    this.position = 0;\n    this.numBuffer = new Uint8Array(8);\n    this.fileInfo = fileInfo ? fileInfo : {};\n  }\n  /**\r\n   * Read a token from the tokenizer-stream\r\n   * @param token - The token to read\r\n   * @param position - If provided, the desired position in the tokenizer-stream\r\n   * @returns Promise with token data\r\n   */\n  async readToken(token, position = this.position) {\n    const uint8Array = Buffer.alloc(token.len);\n    const len = await this.readBuffer(uint8Array, {\n      position\n    });\n    if (len < token.len) throw new peek_readable_1.EndOfStreamError();\n    return token.get(uint8Array, 0);\n  }\n  /**\r\n   * Peek a token from the tokenizer-stream.\r\n   * @param token - Token to peek from the tokenizer-stream.\r\n   * @param position - Offset where to begin reading within the file. If position is null, data will be read from the current file position.\r\n   * @returns Promise with token data\r\n   */\n  async peekToken(token, position = this.position) {\n    const uint8Array = Buffer.alloc(token.len);\n    const len = await this.peekBuffer(uint8Array, {\n      position\n    });\n    if (len < token.len) throw new peek_readable_1.EndOfStreamError();\n    return token.get(uint8Array, 0);\n  }\n  /**\r\n   * Read a numeric token from the stream\r\n   * @param token - Numeric token\r\n   * @returns Promise with number\r\n   */\n  async readNumber(token) {\n    const len = await this.readBuffer(this.numBuffer, {\n      length: token.len\n    });\n    if (len < token.len) throw new peek_readable_1.EndOfStreamError();\n    return token.get(this.numBuffer, 0);\n  }\n  /**\r\n   * Read a numeric token from the stream\r\n   * @param token - Numeric token\r\n   * @returns Promise with number\r\n   */\n  async peekNumber(token) {\n    const len = await this.peekBuffer(this.numBuffer, {\n      length: token.len\n    });\n    if (len < token.len) throw new peek_readable_1.EndOfStreamError();\n    return token.get(this.numBuffer, 0);\n  }\n  /**\r\n   * Ignore number of bytes, advances the pointer in under tokenizer-stream.\r\n   * @param length - Number of bytes to ignore\r\n   * @return resolves the number of bytes ignored, equals length if this available, otherwise the number of bytes available\r\n   */\n  async ignore(length) {\n    if (this.fileInfo.size !== undefined) {\n      const bytesLeft = this.fileInfo.size - this.position;\n      if (length > bytesLeft) {\n        this.position += bytesLeft;\n        return bytesLeft;\n      }\n    }\n    this.position += length;\n    return length;\n  }\n  async close() {\n    // empty\n  }\n  normalizeOptions(uint8Array, options) {\n    if (options && options.position !== undefined && options.position < this.position) {\n      throw new Error('`options.position` must be equal or greater than `tokenizer.position`');\n    }\n    if (options) {\n      return {\n        mayBeLess: options.mayBeLess === true,\n        offset: options.offset ? options.offset : 0,\n        length: options.length ? options.length : uint8Array.length - (options.offset ? options.offset : 0),\n        position: options.position ? options.position : this.position\n      };\n    }\n    return {\n      mayBeLess: false,\n      offset: 0,\n      length: uint8Array.length,\n      position: this.position\n    };\n  }\n}\nexports.AbstractTokenizer = AbstractTokenizer;","map":null,"metadata":{},"sourceType":"script","externalDependencies":[]}